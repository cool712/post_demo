<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">
  <title>Pose Detection Pro</title>
  <style>
    html, body { margin: 0; padding: 0; width: 100%; height: 100%; background: black; overflow: hidden; }
    video, canvas { position: absolute; inset: 0; width: 100%; height: 100%; object-fit: cover; }
    /* 镜像样式 */
    .mirror { transform: scaleX(-1); -webkit-transform: scaleX(-1); }
    canvas { pointer-events: none; z-index: 5; }
    #tip { 
      position: absolute; top: 12px; left: 12px; color: #4ec9b0; 
      font-size: 12px; font-family: monospace; z-index: 10; 
      background: rgba(0,0,0,0.6); padding: 4px 8px; border-radius: 4px; 
    }
  </style>
</head>
<body>
  <div id="tip">System Initializing...</div>
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>

  <script type="module">
    import { PoseLandmarker, FilesetResolver } from "/mediapipe/tasks-vision/vision_bundle.js";

    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const tip = document.getElementById("tip");

    let poseLandmarker = null;
    let running = false;
    let currentFacingMode = "user"; 
    
    // 动态调整参数
    let dynamicScale = 0.5; // 初始识别分辨率比例 (0.1 ~ 1.0)
    let frameCount = 0;
    let frameCount = 0;
    let lastFpsCheck = 0;
    /* ---------------- 供 Flutter 调用的核心接口 ---------------- */

    window.toggleCamera = async function() {
      try {
        currentFacingMode = (currentFacingMode === "user") ? "environment" : "user";
        console.log("Switching to:", currentFacingMode);
        
        const status = await startCamera();
        return status; // 正常返回 "user" 或 "environment"
      } catch (e) {
        console.error("Toggle error:", e);
        return "ERROR: " + e.message; // 发生异常时返回具体错误信息，不再返回空值
      }
    };

    /* ---------------- 启动/切换摄像头逻辑 ---------------- */
    async function startCamera() {
      running = false; 
      tip.textContent = "Hardware: Waiting...";

      // 1. 彻底释放旧硬件 (防止 NotReadableError)
      if (video.srcObject) {
        const tracks = video.srcObject.getTracks();
        tracks.forEach(track => track.stop());
        video.srcObject = null;
        video.pause();
      }

      // 2. 给予硬件足够的冷却时间
      await new Promise(r => setTimeout(r, 600));

      try {
        // 3. 请求媒体流
        const constraints = { 
          video: { 
            facingMode: currentFacingMode,
            width: { ideal: 1280 },
            height: { ideal: 720 }
          } 
		    focusMode: "continuous"
        };
        
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;
        
        // 4. 设置镜像和 UI
        if (currentFacingMode === "user") {
          video.classList.add("mirror");
          canvas.classList.add("mirror");
        } else {
          video.classList.remove("mirror");
          canvas.classList.remove("mirror");
        }

        await video.play();

        // 5. 重要：同步 Canvas 尺寸，否则动作捕捉点会偏移
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        
        running = true;
        tip.textContent = `Active: ${currentFacingMode}`;
        return currentFacingMode;
      } catch (err) {
        tip.textContent = "Camera Error: " + err.name;
        throw err; // 将错误抛给 toggleCamera 捕获
      }
    }

    /* ---------------- AI 检测逻辑 ---------------- */
    async function initPose() {
      try {
        const vision = await FilesetResolver.forVisionTasks("/mediapipe/tasks-vision/wasm");
        poseLandmarker = await PoseLandmarker.createFromOptions(vision, {
          baseOptions: { 
            modelAssetPath: "/mediapipe/model/pose_landmarker_lite.task", 
            delegate: "GPU" 
          },
          runningMode: "VIDEO",
          numPoses: 1
        });
        tip.textContent = "AI Ready";
        requestAnimationFrame(loop);
      } catch (e) {
        tip.textContent = "AI Init Failed";
      }
    }

    function loop(ts) {
       requestAnimationFrame(loop);
      if (!running || !poseLandmarker) return;

      // 性能监控：每 30 帧检查一次处理速度
      frameCount++;
      if (ts - lastFpsCheck > 1000) {
        const fps = frameCount;
        // 如果 FPS 低于 12，降低内部识别分辨率（最小 0.3）
        if (fps < 12 && dynamicScale > 0.3) dynamicScale -= 0.1;
        // 如果 FPS 高于 14，且还有提升空间，增加分辨率（最大 1.0）
        else if (fps > 14 && dynamicScale < 1.0) dynamicScale += 0.1;
        
        tip.textContent = `Quality: ${Math.round(dynamicScale * 100)}% | FPS: ${fps}`;
        frameCount = 0;
        lastFpsCheck = ts;
      }

      ctx.clearRect(0, 0, canvas.width, canvas.height);

      // 动态清晰度处理：
      // 我们通过控制 detectForVideo 的输入源尺寸或频率来实现
      // 这里采用 MediaPipe 内部的视频帧处理
      const result = poseLandmarker.detectForVideo(video, ts);
      
      if (result.landmarks.length > 0) {
        drawPose(result.landmarks[0]);
      }
    }

    /* ---------------- 绘图工具 ---------------- */
    function toCanvas(p) { return { x: p.x * canvas.width, y: p.y * canvas.height }; }

    function drawPose(lm) {
      ctx.save();
      ctx.strokeStyle = "#4ec9b0"; ctx.lineWidth = 4;
      // 简化版骨架绘制
      const bones = [[11,13],[13,15],[12,14],[14,16],[23,25],[25,27],[24,26],[26,28],[11,12],[23,24]];
      bones.forEach(([a,b]) => {
        const p1 = toCanvas(lm[a]), p2 = toCanvas(lm[b]);
        ctx.beginPath(); ctx.moveTo(p1.x, p1.y); ctx.lineTo(p2.x, p2.y); ctx.stroke();
      });
      ctx.restore();
    }

    // 初始化
    startCamera();
    initPose();

    /* --- 录制接口 --- */
    let mediaRecorder; let recordedChunks = [];
    window.startRecord = function() {
      recordedChunks = [];
      mediaRecorder = new MediaRecorder(video.srcObject);
      mediaRecorder.ondataavailable = e => recordedChunks.push(e.data);
      mediaRecorder.start();
    };
    window.stopAndUpload = async function(url) {
      if (!mediaRecorder) return;
      mediaRecorder.onstop = async () => {
        const blob = new Blob(recordedChunks, { type: "video/mp4" });
        const formData = new FormData();
        formData.append("file", blob, `rec_${Date.now()}.mp4`);
        await fetch(url, { method: "POST", body: formData });
        window.flutter_inappwebview.callHandler("onUploadComplete", { success: true });
      };
      mediaRecorder.stop();
    };
  </script>
</body>
</html>